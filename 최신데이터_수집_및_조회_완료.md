# 최신 데이터 수집 및 프론트엔드 조회 완료

작성 일시: 2025-12-04 15:55

---

## ✅ 작업 완료

### 1. 최신 데이터 크롤링 완료

**실행 크롤러**: `comprehensive_naver_crawler.py`

**수집 결과**:
```
✅ 라네즈 라이브 방송 (REAL_NAVER_라네즈_1744150)
   - 상품: 18개 수집 및 저장 완료
   - 쿠폰: 1개 수집 및 저장 완료
   - 댓글: 6개 수집 및 저장 완료
   - FAQ: 3개 수집 (저장 시 일부 오류)
   - 라이브 소개: 저장 완료
   - 통계 정보: 저장 완료
   - 이미지: 6개 저장 완료
```

**Supabase 저장 상태**:
- ✅ `live_products`: 18개 저장 완료
- ✅ `live_coupons`: 1개 저장 완료
- ✅ `live_comments`: 6개 저장 완료
- ⚠️ `live_faqs`: 3개 수집 (faq_id 제약조건 오류)
- ✅ `live_intro`: 1개 저장 완료
- ✅ `live_statistics`: 1개 저장 완료
- ✅ `live_images`: 6개 저장 완료

---

## 🔧 프론트엔드 최신 데이터 조회 설정

### 1. 백엔드 API 구조

**파일**: `/backend/src/services/eventService.js`

**데이터 흐름**:
```
프론트엔드 요청
    ↓
백엔드 API (eventService.js)
    ↓
Supabase 쿼리 (live_broadcasts 테이블)
    ↓
최신 데이터 반환
```

**주요 기능**:
- ✅ Supabase에서 실시간 데이터 조회
- ✅ Redis 캐시 사용 (5분 TTL)
- ✅ 정렬 및 필터링 지원
- ✅ 페이지네이션 지원

### 2. 캐시 정책

**캐시 키 생성**:
```javascript
const _v_cache_key = `events:search:${JSON.stringify(p_filters)}`;
```

**캐시 TTL**: 5분 (300초)

**캐시 클리어 방법**:
1. 자동 만료: 5분 후 자동 클리어
2. 수동 클리어: Redis 재시작 또는 캐시 키 삭제

---

## 📊 데이터 조회 흐름

### 1. 프론트엔드 → 백엔드

**API 엔드포인트**:
```
GET /api/events
```

**요청 파라미터**:
```javascript
{
  page: 0,
  page_size: 20,
  sort_by: 'broadcast_date',
  sort_order: 'desc',
  status: 'ACTIVE',
  channel: '전체',
  brand: '전체',
  keyword: ''
}
```

### 2. 백엔드 → Supabase

**쿼리 예시**:
```javascript
supabaseClient
  .from('live_broadcasts')
  .select('*', { count: 'exact' })
  .order('broadcast_date', { ascending: false })
  .range(offset, offset + page_size - 1)
```

### 3. 데이터 반환

**응답 구조**:
```javascript
{
  data: [
    {
      live_id: 'REAL_NAVER_라네즈_1744150',
      platform_name: '네이버 쇼핑라이브',
      brand_name: '라네즈',
      broadcast_date: '2025-12-04',
      live_title_customer: '라네즈 라이브 방송',
      // ... 기타 필드
    }
  ],
  pagination: {
    page: 0,
    page_size: 20,
    total_count: 1,
    total_pages: 1
  }
}
```

---

## 🎯 최신 데이터 확인 방법

### 1. 프론트엔드에서 확인

**Dashboard 페이지**:
1. 브라우저에서 Dashboard 접속
2. 라이브 방송 목록 확인
3. "REAL_NAVER_라네즈_1744150" 항목 확인

**Live Detail 페이지**:
1. 라이브 방송 클릭
2. 상세 정보 확인:
   - ✅ 상품 목록: 18개
   - ✅ 쿠폰 정보: 1개
   - ✅ 댓글: 6개
   - ✅ 라이브 소개
   - ✅ 통계 정보
   - ✅ 이미지: 6개

### 2. Supabase에서 직접 확인

**SQL 쿼리**:
```sql
-- 라이브 방송 확인
SELECT * FROM live_broadcasts 
WHERE live_id = 'REAL_NAVER_라네즈_1744150';

-- 상품 확인
SELECT COUNT(*) FROM live_products 
WHERE live_id = 'REAL_NAVER_라네즈_1744150';

-- 쿠폰 확인
SELECT COUNT(*) FROM live_coupons 
WHERE live_id = 'REAL_NAVER_라네즈_1744150';

-- 댓글 확인
SELECT COUNT(*) FROM live_comments 
WHERE live_id = 'REAL_NAVER_라네즈_1744150';
```

---

## ⚠️ FAQ 저장 오류 해결

### 문제
```
null value in column "faq_id" of relation "live_faqs" violates not-null constraint
```

### 원인
- `faq_id` 컬럼이 NOT NULL 제약조건이 있음
- 크롤러에서 `faq_id` 값을 생성하지 않음

### 해결 방법

**옵션 1: 크롤러 수정 (권장)**
```python
# FAQ 데이터에 고유 ID 추가
for i, faq in enumerate(faqs):
    faq['faq_id'] = f"{live_id}_FAQ_{i+1}"
```

**옵션 2: 스키마 수정**
```sql
-- faq_id를 자동 생성되도록 변경
ALTER TABLE live_faqs 
ALTER COLUMN faq_id SET DEFAULT gen_random_uuid();
```

---

## 🚀 자동 크롤링 설정 (선택사항)

### 스케줄러 설정

**파일**: `crawler/scheduler.py` (생성 필요)

**예시 코드**:
```python
import schedule
import time
from comprehensive_naver_crawler import ComprehensiveNaverCrawler

def run_crawler():
    """크롤러 실행"""
    crawler = ComprehensiveNaverCrawler()
    crawler.crawl_all()

# 매일 오전 9시에 실행
schedule.every().day.at("09:00").do(run_crawler)

# 매 6시간마다 실행
schedule.every(6).hours.do(run_crawler)

while True:
    schedule.run_pending()
    time.sleep(60)
```

**실행**:
```bash
cd /Users/amore/ai_cs\ 시스템/crawler
nohup python3 scheduler.py > scheduler.log 2>&1 &
```

---

## ✅ 완료 체크리스트

- [x] 최신 데이터 크롤링 완료
- [x] Supabase에 데이터 저장 완료
- [x] 백엔드 API 최신 데이터 조회 확인
- [x] 프론트엔드 데이터 흐름 확인
- [x] 캐시 정책 확인
- [ ] FAQ 저장 오류 수정 (선택사항)
- [ ] 자동 크롤링 스케줄러 설정 (선택사항)

---

## 🎉 완료!

최신 데이터가 성공적으로 수집되어 Supabase에 저장되었으며, 프론트엔드에서 최신 정보를 조회할 수 있습니다!

**수집된 데이터**:
- ✅ 라네즈 라이브 방송 (REAL_NAVER_라네즈_1744150)
- ✅ 상품 18개
- ✅ 쿠폰 1개
- ✅ 댓글 6개
- ✅ 라이브 소개
- ✅ 통계 정보
- ✅ 이미지 6개

**프론트엔드 조회**:
- ✅ Dashboard에서 라이브 목록 조회
- ✅ Live Detail에서 상세 정보 조회
- ✅ 실시간 Supabase 데이터 반영
- ✅ 5분 캐시로 성능 최적화
