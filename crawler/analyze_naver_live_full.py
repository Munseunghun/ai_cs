#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ì‡¼í•‘ë¼ì´ë¸Œ ì „ì²´ ì •ë³´ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
ìƒí’ˆ, ì¿ í°, í˜œíƒ, ëŒ“ê¸€, ì±„íŒ…, FAQ ë“± ëª¨ë“  ì •ë³´ ìˆ˜ì§‘
"""

import time
import json
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

def analyze_full_page(p_url):
    """ë„¤ì´ë²„ ì‡¼í•‘ë¼ì´ë¸Œ ì „ì²´ ì •ë³´ ë¶„ì„"""
    
    # Chrome ì˜µì…˜ ì„¤ì •
    options = Options()
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--window-size=1920,1080')
    options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')
    
    # ë“œë¼ì´ë²„ ì´ˆê¸°í™”
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    
    try:
        print(f"ğŸ” í˜ì´ì§€ ë¡œë“œ ì¤‘: {p_url}")
        driver.get(p_url)
        time.sleep(8)
        
        # ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ì½˜í…ì¸  ë¡œë“œ
        print("ğŸ“œ ìŠ¤í¬ë¡¤í•˜ì—¬ ì½˜í…ì¸  ë¡œë“œ ì¤‘...")
        for i in range(5):
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
        
        # ìƒë‹¨ìœ¼ë¡œ ìŠ¤í¬ë¡¤
        driver.execute_script("window.scrollTo(0, 0);")
        time.sleep(2)
        
        # HTML íŒŒì‹±
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        
        print("\n" + "=" * 80)
        print("ğŸ“Š ì „ì²´ ì •ë³´ ë¶„ì„ ê²°ê³¼")
        print("=" * 80)
        
        # 1. ìƒí’ˆ ì •ë³´
        print("\n1ï¸âƒ£ ìƒí’ˆ ì •ë³´:")
        products = soup.select('[class*="product"], [class*="Product"], [class*="item"], [class*="Item"]')
        print(f"   - ìƒí’ˆ ê´€ë ¨ ìš”ì†Œ: {len(products)}ê°œ")
        
        # ìƒí’ˆ ê°€ê²© ì •ë³´
        prices = soup.select('[class*="price"], [class*="Price"]')
        print(f"   - ê°€ê²© ì •ë³´: {len(prices)}ê°œ")
        for price in prices[:5]:
            print(f"     * {price.get_text(strip=True)[:50]}")
        
        # 2. ì¿ í° ì •ë³´
        print("\n2ï¸âƒ£ ì¿ í° ì •ë³´:")
        coupons = soup.select('[class*="coupon"], [class*="Coupon"], [class*="benefit"], [class*="Benefit"]')
        print(f"   - ì¿ í°/í˜œíƒ ìš”ì†Œ: {len(coupons)}ê°œ")
        for coupon in coupons[:5]:
            text = coupon.get_text(strip=True)
            if text and len(text) > 5:
                print(f"     * {text[:80]}")
        
        # 3. ë¼ì´ë¸Œ ì†Œê°œ
        print("\n3ï¸âƒ£ ë¼ì´ë¸Œ ì†Œê°œ:")
        descriptions = soup.select('[class*="description"], [class*="Description"], [class*="intro"], [class*="Intro"]')
        print(f"   - ì†Œê°œ ìš”ì†Œ: {len(descriptions)}ê°œ")
        for desc in descriptions[:3]:
            text = desc.get_text(strip=True)
            if text and len(text) > 10:
                print(f"     * {text[:100]}")
        
        # 4. ëŒ“ê¸€/ì±„íŒ…
        print("\n4ï¸âƒ£ ëŒ“ê¸€/ì±„íŒ…:")
        comments = soup.select('[class*="comment"], [class*="Comment"], [class*="chat"], [class*="Chat"], [class*="message"], [class*="Message"]')
        print(f"   - ëŒ“ê¸€/ì±„íŒ… ìš”ì†Œ: {len(comments)}ê°œ")
        for comment in comments[:5]:
            text = comment.get_text(strip=True)
            if text and len(text) > 3:
                print(f"     * {text[:80]}")
        
        # 5. ë²„íŠ¼ ë° ì•¡ì…˜
        print("\n5ï¸âƒ£ ë²„íŠ¼ ë° ì•¡ì…˜:")
        buttons = soup.select('button, [role="button"]')
        print(f"   - ë²„íŠ¼: {len(buttons)}ê°œ")
        button_texts = set()
        for btn in buttons:
            text = btn.get_text(strip=True)
            if text and len(text) < 30:
                button_texts.add(text)
        for text in sorted(button_texts)[:20]:
            print(f"     * {text}")
        
        # 6. ì´ë¯¸ì§€
        print("\n6ï¸âƒ£ ì´ë¯¸ì§€:")
        images = soup.select('img')
        print(f"   - ì´ë¯¸ì§€: {len(images)}ê°œ")
        for img in images[:5]:
            src = img.get('src', '')
            alt = img.get('alt', '')
            if src:
                print(f"     * {alt[:30] if alt else 'No alt'}: {src[:80]}")
        
        # 7. ë°ì´í„° ì†ì„±
        print("\n7ï¸âƒ£ ë°ì´í„° ì†ì„±:")
        data_elements = soup.select('[data-product-id], [data-product-no], [data-item-id]')
        print(f"   - ì œí’ˆ ID ì†ì„±: {len(data_elements)}ê°œ")
        for elem in data_elements[:5]:
            attrs = {k: v for k, v in elem.attrs.items() if k.startswith('data-')}
            print(f"     * {attrs}")
        
        # 8. JSON ë°ì´í„° ì¶”ì¶œ ì‹œë„
        print("\n8ï¸âƒ£ í˜ì´ì§€ ë‚´ JSON ë°ì´í„°:")
        scripts = soup.find_all('script', type='application/json')
        print(f"   - JSON ìŠ¤í¬ë¦½íŠ¸: {len(scripts)}ê°œ")
        
        for idx, script in enumerate(scripts[:3], 1):
            try:
                data = json.loads(script.string)
                print(f"   JSON {idx}:")
                if isinstance(data, dict):
                    print(f"     í‚¤: {list(data.keys())[:10]}")
                elif isinstance(data, list):
                    print(f"     ë°°ì—´ ê¸¸ì´: {len(data)}")
            except:
                pass
        
        # 9. íŠ¹ì • í´ë˜ìŠ¤ íŒ¨í„´ ê²€ìƒ‰
        print("\n9ï¸âƒ£ ì£¼ìš” í´ë˜ìŠ¤ íŒ¨í„´:")
        important_patterns = ['product', 'coupon', 'benefit', 'comment', 'chat', 'faq', 'question']
        for pattern in important_patterns:
            elements = soup.select(f'[class*="{pattern}"], [class*="{pattern.capitalize()}"]')
            if elements:
                print(f"   - {pattern}: {len(elements)}ê°œ")
        
        # 10. ë©”íƒ€ ì •ë³´
        print("\nğŸ”Ÿ ë©”íƒ€ ì •ë³´:")
        title = soup.find('meta', property='og:title')
        desc = soup.find('meta', property='og:description')
        image = soup.find('meta', property='og:image')
        
        if title:
            print(f"   - ì œëª©: {title.get('content', '')[:80]}")
        if desc:
            print(f"   - ì„¤ëª…: {desc.get('content', '')[:80]}")
        if image:
            print(f"   - ì´ë¯¸ì§€: {image.get('content', '')[:80]}")
        
        # HTML ì €ì¥
        output_file = '/Users/amore/ai_cs ì‹œìŠ¤í…œ/crawler/logs/naver_live_full_analysis.html'
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(html)
        print(f"\nğŸ’¾ ì „ì²´ HTML ì €ì¥: {output_file}")
        
        # ìŠ¤í¬ë¦°ìƒ·
        screenshot_file = '/Users/amore/ai_cs ì‹œìŠ¤í…œ/crawler/logs/naver_live_screenshot.png'
        driver.save_screenshot(screenshot_file)
        print(f"ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_file}")
        
        print("\n" + "=" * 80)
        
    finally:
        driver.quit()
        print("âœ… ë¸Œë¼ìš°ì € ì¢…ë£Œ")


if __name__ == '__main__':
    # ìƒ˜í”Œ URL
    test_url = "https://view.shoppinglive.naver.com/replays/1744150?fm=shoppinglive&sn=home&tr=lim"
    
    print("ğŸ¬ ë„¤ì´ë²„ ì‡¼í•‘ë¼ì´ë¸Œ ì „ì²´ ì •ë³´ ë¶„ì„ ì‹œì‘")
    print("=" * 80)
    
    analyze_full_page(test_url)
    
    print("\nâœ… ë¶„ì„ ì™„ë£Œ!")
