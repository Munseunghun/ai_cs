#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ì‡¼í•‘ë¼ì´ë¸Œ STT ì •ë³´ ìˆ˜ì§‘ í¬ë¡¤ëŸ¬
ë¼ì´ë¸Œ ë°©ì†¡ì˜ íŠ¹í™” ì •ë³´(íƒ€ì„ë¼ì¸, ì£¼ìš” ë©˜íŠ¸, Q&A ë“±)ë¥¼ ìˆ˜ì§‘
"""

import sys
import time
import json
import logging
import re
from datetime import datetime
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

# ë¡œì»¬ ëª¨ë“ˆ
sys.path.append('.')
from supabase import create_client
from dotenv import load_dotenv
import os

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class NaverSTTCrawler:
    """ë„¤ì´ë²„ ì‡¼í•‘ë¼ì´ë¸Œ STT ì •ë³´ ìˆ˜ì§‘ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.driver = None
        
        # Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        supabase_url = os.getenv('SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_ANON_KEY')
        
        if not supabase_url or not supabase_key:
            logger.error("âŒ Supabase ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            raise ValueError("Supabase ì„¤ì • í•„ìš”")
        
        self.supabase = create_client(supabase_url, supabase_key)
        logger.info("âœ… Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        self.stats = {
            'total_processed': 0,
            'total_stt_collected': 0,
            'total_saved': 0,
            'errors': []
        }
    
    def init_driver(self):
        """Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
        try:
            options = Options()
            # headless ëª¨ë“œ ë¹„í™œì„±í™” (STT ì •ë³´ ë¡œë“œë¥¼ ìœ„í•´)
            # options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument('--window-size=1920,1080')
            options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=options)
            logger.info("âœ… ChromeDriver ì´ˆê¸°í™” ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"âŒ ChromeDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def close_driver(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            logger.info("ChromeDriver ì¢…ë£Œ")
    
    def extract_stt_info(self, p_live_url, p_live_id):
        """
        ë¼ì´ë¸Œ ë°©ì†¡ í˜ì´ì§€ì—ì„œ STT ê¸°ë°˜ ì •ë³´ ì¶”ì¶œ
        
        Args:
            p_live_url (str): ë¼ì´ë¸Œ ë°©ì†¡ URL
            p_live_id (str): ë¼ì´ë¸Œ ë°©ì†¡ ID
            
        Returns:
            dict: STT ì •ë³´ (íƒ€ì„ë¼ì¸, ì£¼ìš” ë©˜íŠ¸, Q&A ë“±)
        """
        try:
            logger.info(f"   ğŸ¤ STT ì •ë³´ ìˆ˜ì§‘ ì¤‘: {p_live_id}")
            
            # í˜ì´ì§€ ë¡œë“œ
            self.driver.get(p_live_url)
            time.sleep(5)  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            
            # ìŠ¤í¬ë¡¤í•˜ì—¬ ëª¨ë“  ì½˜í…ì¸  ë¡œë“œ
            for i in range(3):
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
            
            # HTML íŒŒì‹±
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            stt_info = {
                'live_id': p_live_id,
                'key_message': [],  # ì£¼ìš” ë©˜íŠ¸/ë©”ì‹œì§€
                'broadcast_qa': [],  # ë°©ì†¡ ì¤‘ Q&A
                'timeline_summary': [],  # íƒ€ì„ë¼ì¸ ìš”ì•½
                'product_mentions': [],  # ì œí’ˆ ì–¸ê¸‰
                'host_comments': [],  # ì§„í–‰ì ì½”ë©˜íŠ¸
                'viewer_reactions': [],  # ì‹œì²­ì ë°˜ì‘
                'collected_at': datetime.now().isoformat()
            }
            
            # 1. íƒ€ì„ë¼ì¸ ì •ë³´ ìˆ˜ì§‘
            timeline_items = self._extract_timeline(soup)
            if timeline_items:
                stt_info['timeline_summary'] = timeline_items
                logger.info(f"      âœ… íƒ€ì„ë¼ì¸: {len(timeline_items)}ê°œ")
            
            # 2. ì£¼ìš” ë©˜íŠ¸/í‚¤ ë©”ì‹œì§€ ìˆ˜ì§‘
            key_messages = self._extract_key_messages(soup)
            if key_messages:
                stt_info['key_message'] = key_messages
                logger.info(f"      âœ… ì£¼ìš” ë©˜íŠ¸: {len(key_messages)}ê°œ")
            
            # 3. ì œí’ˆ ì–¸ê¸‰ ìˆ˜ì§‘
            product_mentions = self._extract_product_mentions(soup)
            if product_mentions:
                stt_info['product_mentions'] = product_mentions
                logger.info(f"      âœ… ì œí’ˆ ì–¸ê¸‰: {len(product_mentions)}ê°œ")
            
            # 4. ëŒ“ê¸€/ì±„íŒ… ë¶„ì„ (Q&A ì¶”ì¶œ)
            qa_items = self._extract_qa_from_comments(soup)
            if qa_items:
                stt_info['broadcast_qa'] = qa_items
                logger.info(f"      âœ… Q&A: {len(qa_items)}ê°œ")
            
            # 5. ì§„í–‰ì ì½”ë©˜íŠ¸ ìˆ˜ì§‘
            host_comments = self._extract_host_comments(soup)
            if host_comments:
                stt_info['host_comments'] = host_comments
                logger.info(f"      âœ… ì§„í–‰ì ì½”ë©˜íŠ¸: {len(host_comments)}ê°œ")
            
            # 6. ì‹œì²­ì ë°˜ì‘ ìˆ˜ì§‘
            viewer_reactions = self._extract_viewer_reactions(soup)
            if viewer_reactions:
                stt_info['viewer_reactions'] = viewer_reactions
                logger.info(f"      âœ… ì‹œì²­ì ë°˜ì‘: {len(viewer_reactions)}ê°œ")
            
            return stt_info
            
        except Exception as e:
            logger.error(f"   âŒ STT ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_timeline(self, p_soup):
        """íƒ€ì„ë¼ì¸ ì •ë³´ ì¶”ì¶œ"""
        timeline_items = []
        
        try:
            # ë„¤ì´ë²„ ì‡¼í•‘ë¼ì´ë¸Œì˜ íƒ€ì„ë¼ì¸ ì„ íƒì
            # (ì‹¤ì œ ì„ íƒìëŠ” í˜ì´ì§€ êµ¬ì¡°ì— ë”°ë¼ ì¡°ì • í•„ìš”)
            timeline_elements = p_soup.select('[class*="timeline"], [class*="chapter"], [class*="segment"]')
            
            for element in timeline_elements:
                # ì‹œê°„ ì •ë³´
                time_elem = element.select_one('[class*="time"], [class*="timestamp"]')
                time_str = time_elem.get_text(strip=True) if time_elem else None
                
                # ë‚´ìš©
                content_elem = element.select_one('[class*="title"], [class*="content"], [class*="description"]')
                content = content_elem.get_text(strip=True) if content_elem else None
                
                if time_str and content:
                    timeline_items.append({
                        'timestamp': time_str,
                        'content': content,
                        'type': 'timeline'
                    })
            
            # ëŒ€ì²´ ë°©ë²•: ë¹„ë””ì˜¤ ì±•í„° ì •ë³´
            if not timeline_items:
                chapter_elements = p_soup.select('[data-chapter], [data-timestamp]')
                for element in chapter_elements:
                    timestamp = element.get('data-timestamp') or element.get('data-chapter')
                    content = element.get_text(strip=True)
                    if timestamp and content:
                        timeline_items.append({
                            'timestamp': timestamp,
                            'content': content,
                            'type': 'chapter'
                        })
        
        except Exception as e:
            logger.warning(f"      íƒ€ì„ë¼ì¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return timeline_items
    
    def _extract_key_messages(self, p_soup):
        """ì£¼ìš” ë©˜íŠ¸/í‚¤ ë©”ì‹œì§€ ì¶”ì¶œ"""
        key_messages = []
        
        try:
            # í•˜ì´ë¼ì´íŠ¸, ì£¼ìš” ë©˜íŠ¸ ì„ íƒì
            highlight_elements = p_soup.select('[class*="highlight"], [class*="key"], [class*="important"]')
            
            for element in highlight_elements:
                message = element.get_text(strip=True)
                if message and len(message) > 10:  # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì˜ ë©”ì‹œì§€ë§Œ
                    key_messages.append({
                        'message': message,
                        'type': 'highlight',
                        'length': len(message)
                    })
            
            # ëŒ€ì²´ ë°©ë²•: ê°•ì¡°ëœ í…ìŠ¤íŠ¸ (bold, strong íƒœê·¸)
            if not key_messages:
                emphasized = p_soup.select('strong, b, [class*="emphasis"]')
                for elem in emphasized:
                    message = elem.get_text(strip=True)
                    if message and len(message) > 10 and len(message) < 200:
                        key_messages.append({
                            'message': message,
                            'type': 'emphasized',
                            'length': len(message)
                        })
        
        except Exception as e:
            logger.warning(f"      ì£¼ìš” ë©˜íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return key_messages[:20]  # ìµœëŒ€ 20ê°œ
    
    def _extract_product_mentions(self, p_soup):
        """ì œí’ˆ ì–¸ê¸‰ ì •ë³´ ì¶”ì¶œ"""
        product_mentions = []
        
        try:
            # ì œí’ˆ ê´€ë ¨ ì„ íƒì
            product_elements = p_soup.select('[class*="product"], [class*="item"], [data-product-id]')
            
            for element in product_elements:
                # ì œí’ˆëª…
                name_elem = element.select_one('[class*="name"], [class*="title"]')
                product_name = name_elem.get_text(strip=True) if name_elem else None
                
                # ê°€ê²©
                price_elem = element.select_one('[class*="price"]')
                price = price_elem.get_text(strip=True) if price_elem else None
                
                # ì œí’ˆ ID
                product_id = element.get('data-product-id')
                
                if product_name:
                    product_mentions.append({
                        'product_name': product_name,
                        'price': price,
                        'product_id': product_id,
                        'type': 'product_mention'
                    })
        
        except Exception as e:
            logger.warning(f"      ì œí’ˆ ì–¸ê¸‰ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return product_mentions[:30]  # ìµœëŒ€ 30ê°œ
    
    def _extract_qa_from_comments(self, p_soup):
        """ëŒ“ê¸€/ì±„íŒ…ì—ì„œ Q&A ì¶”ì¶œ"""
        qa_items = []
        
        try:
            # ëŒ“ê¸€/ì±„íŒ… ì„ íƒì
            comment_elements = p_soup.select('[class*="comment"], [class*="chat"], [class*="message"]')
            
            for element in comment_elements:
                text = element.get_text(strip=True)
                
                # ì§ˆë¬¸ íŒ¨í„´ ê°ì§€ (?, ì–´ë–»ê²Œ, ì–¸ì œ, ë­, ì–´ë”” ë“±)
                if any(keyword in text for keyword in ['?', 'ì–´ë–»ê²Œ', 'ì–¸ì œ', 'ë­', 'ì–´ë””', 'ì–¼ë§ˆ', 'ì¶”ì²œ']):
                    qa_items.append({
                        'question': text,
                        'type': 'user_question',
                        'detected_pattern': 'question_keyword'
                    })
        
        except Exception as e:
            logger.warning(f"      Q&A ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return qa_items[:15]  # ìµœëŒ€ 15ê°œ
    
    def _extract_host_comments(self, p_soup):
        """ì§„í–‰ì ì½”ë©˜íŠ¸ ì¶”ì¶œ"""
        host_comments = []
        
        try:
            # ì§„í–‰ì ê´€ë ¨ ì„ íƒì
            host_elements = p_soup.select('[class*="host"], [class*="presenter"], [class*="mc"]')
            
            for element in host_elements:
                comment = element.get_text(strip=True)
                if comment and len(comment) > 10:
                    host_comments.append({
                        'comment': comment,
                        'type': 'host_comment',
                        'length': len(comment)
                    })
        
        except Exception as e:
            logger.warning(f"      ì§„í–‰ì ì½”ë©˜íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return host_comments[:20]  # ìµœëŒ€ 20ê°œ
    
    def _extract_viewer_reactions(self, p_soup):
        """ì‹œì²­ì ë°˜ì‘ ì¶”ì¶œ"""
        viewer_reactions = []
        
        try:
            # ì¢‹ì•„ìš”, í•˜íŠ¸, ì´ëª¨ì§€ ë“±
            reaction_elements = p_soup.select('[class*="reaction"], [class*="like"], [class*="heart"]')
            
            for element in reaction_elements:
                reaction_type = element.get('class', [''])[0]
                count_elem = element.select_one('[class*="count"]')
                count = count_elem.get_text(strip=True) if count_elem else '0'
                
                viewer_reactions.append({
                    'reaction_type': reaction_type,
                    'count': count,
                    'type': 'viewer_reaction'
                })
        
        except Exception as e:
            logger.warning(f"      ì‹œì²­ì ë°˜ì‘ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return viewer_reactions
    
    def save_stt_info(self, p_stt_info):
        """
        STT ì •ë³´ë¥¼ Supabaseì— ì €ì¥
        
        Args:
            p_stt_info (dict): STT ì •ë³´
            
        Returns:
            bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            if not p_stt_info or not p_stt_info.get('live_id'):
                logger.warning("   âš ï¸ STT ì •ë³´ê°€ ë¹„ì–´ìˆê±°ë‚˜ live_idê°€ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # JSON ì§ë ¬í™”
            data_to_save = {
                'live_id': p_stt_info['live_id'],
                'key_message': json.dumps(p_stt_info.get('key_message', []), ensure_ascii=False),
                'broadcast_qa': json.dumps(p_stt_info.get('broadcast_qa', []), ensure_ascii=False),
                'timeline_summary': json.dumps(p_stt_info.get('timeline_summary', []), ensure_ascii=False),
                'product_mentions': json.dumps(p_stt_info.get('product_mentions', []), ensure_ascii=False),
                'host_comments': json.dumps(p_stt_info.get('host_comments', []), ensure_ascii=False),
                'viewer_reactions': json.dumps(p_stt_info.get('viewer_reactions', []), ensure_ascii=False),
                'collected_at': p_stt_info.get('collected_at'),
                'updated_at': datetime.now().isoformat()
            }
            
            # UPSERT (ì¤‘ë³µ ì‹œ ì—…ë°ì´íŠ¸)
            response = self.supabase.table('live_stt_info').upsert(
                data_to_save,
                on_conflict='live_id'
            ).execute()
            
            if response.data:
                logger.info(f"   âœ… STT ì •ë³´ ì €ì¥ ì™„ë£Œ: {p_stt_info['live_id']}")
                return True
            else:
                logger.error(f"   âŒ STT ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {response}")
                return False
                
        except Exception as e:
            logger.error(f"   âŒ STT ì •ë³´ ì €ì¥ ì¤‘ ì—ëŸ¬: {e}")
            return False
    
    def crawl_existing_lives(self, p_limit=50):
        """
        ê¸°ì¡´ live_broadcastsì—ì„œ STT ì •ë³´ê°€ ì—†ëŠ” ë¼ì´ë¸Œ ë°©ì†¡ì„ ì°¾ì•„ ìˆ˜ì§‘
        
        Args:
            p_limit (int): ìµœëŒ€ ì²˜ë¦¬ ê°œìˆ˜
        """
        try:
            logger.info(f"ğŸ¯ STT ì •ë³´ ìˆ˜ì§‘ ì‹œì‘ (ìµœëŒ€ {p_limit}ê°œ)")
            
            # STT ì •ë³´ê°€ ì—†ëŠ” ë¼ì´ë¸Œ ë°©ì†¡ ì¡°íšŒ
            response = self.supabase.table('live_broadcasts').select(
                'live_id, source_url, brand_name, live_title_customer'
            ).is_('source_url', 'not.null').limit(p_limit).execute()
            
            if not response.data:
                logger.warning("âš ï¸ ì²˜ë¦¬í•  ë¼ì´ë¸Œ ë°©ì†¡ì´ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            lives = response.data
            logger.info(f"ğŸ“‹ ì´ {len(lives)}ê°œ ë¼ì´ë¸Œ ë°©ì†¡ ë°œê²¬")
            
            # ì´ë¯¸ STT ì •ë³´ê°€ ìˆëŠ” live_id ì¡°íšŒ
            existing_stt = self.supabase.table('live_stt_info').select('live_id').execute()
            existing_live_ids = {item['live_id'] for item in existing_stt.data} if existing_stt.data else set()
            
            logger.info(f"ğŸ“Š ì´ë¯¸ STT ì •ë³´ê°€ ìˆëŠ” ë°©ì†¡: {len(existing_live_ids)}ê°œ")
            
            # ë“œë¼ì´ë²„ ì´ˆê¸°í™”
            if not self.init_driver():
                logger.error("âŒ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨")
                return
            
            # ê° ë¼ì´ë¸Œ ë°©ì†¡ ì²˜ë¦¬
            for idx, live in enumerate(lives, 1):
                live_id = live['live_id']
                source_url = live['source_url']
                brand_name = live.get('brand_name', 'Unknown')
                title = live.get('live_title_customer', 'No Title')
                
                # ì´ë¯¸ STT ì •ë³´ê°€ ìˆìœ¼ë©´ ìŠ¤í‚µ
                if live_id in existing_live_ids:
                    logger.info(f"[{idx}/{len(lives)}] â­ï¸ ìŠ¤í‚µ (ì´ë¯¸ ì¡´ì¬): {live_id}")
                    continue
                
                logger.info(f"[{idx}/{len(lives)}] ğŸ¬ ì²˜ë¦¬ ì¤‘: [{brand_name}] {title[:50]}")
                
                # STT ì •ë³´ ìˆ˜ì§‘
                stt_info = self.extract_stt_info(source_url, live_id)
                
                if stt_info:
                    # ì €ì¥
                    if self.save_stt_info(stt_info):
                        self.stats['total_stt_collected'] += 1
                        self.stats['total_saved'] += 1
                    else:
                        self.stats['errors'].append({
                            'live_id': live_id,
                            'error': 'Save failed'
                        })
                else:
                    logger.warning(f"   âš ï¸ STT ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {live_id}")
                    self.stats['errors'].append({
                        'live_id': live_id,
                        'error': 'Extraction failed'
                    })
                
                self.stats['total_processed'] += 1
                
                # ìš”ì²­ ê°„ ëŒ€ê¸° (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                time.sleep(3)
            
            # ë“œë¼ì´ë²„ ì¢…ë£Œ
            self.close_driver()
            
            # ìµœì¢… í†µê³„
            logger.info("=" * 80)
            logger.info("ğŸ‰ STT ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ!")
            logger.info(f"   - ì²˜ë¦¬í•œ ë°©ì†¡: {self.stats['total_processed']}ê°œ")
            logger.info(f"   - STT ìˆ˜ì§‘ ì„±ê³µ: {self.stats['total_stt_collected']}ê°œ")
            logger.info(f"   - ì €ì¥ ì„±ê³µ: {self.stats['total_saved']}ê°œ")
            logger.info(f"   - ì—ëŸ¬: {len(self.stats['errors'])}ê°œ")
            logger.info("=" * 80)
            
        except Exception as e:
            logger.error(f"âŒ STT ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {e}")
            self.close_driver()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        crawler = NaverSTTCrawler()
        
        # ìµœëŒ€ 50ê°œ ë¼ì´ë¸Œ ë°©ì†¡ì˜ STT ì •ë³´ ìˆ˜ì§‘
        crawler.crawl_existing_lives(p_limit=50)
        
    except Exception as e:
        logger.error(f"âŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return 1
    
    return 0


if __name__ == '__main__':
    sys.exit(main())
