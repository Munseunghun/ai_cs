#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ê°œì„ ëœ ë©€í‹° í”Œë«í¼ í¬ë¡¤ëŸ¬
10ê°œ í”Œë«í¼ Ã— 10ê°œ ë¸Œëœë“œì˜ ì‹¤ì œ ë¼ì´ë¸Œ ì‡¼í•‘ ë°ì´í„° ìˆ˜ì§‘
"""

import sys
import time
import json
import logging
from datetime import datetime
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import urllib.parse

# ë¡œì»¬ ëª¨ë“ˆ
sys.path.append('.')
from supabase import create_client
from dotenv import load_dotenv
import os

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ì„¤ì • ë¡œë“œ
config_dir = Path(__file__).parent / 'config'
with open(config_dir / 'platforms.json', 'r', encoding='utf-8') as f:
    PLATFORMS = json.load(f)

with open(config_dir / 'brands.json', 'r', encoding='utf-8') as f:
    BRANDS = json.load(f)


class ImprovedMultiPlatformCrawler:
    """ê°œì„ ëœ ë©€í‹° í”Œë«í¼ í¬ë¡¤ëŸ¬"""
    
    def __init__(self):
        """í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.driver = None
        
        # Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        supabase_url = os.getenv('SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_ANON_KEY')
        
        if not supabase_url or not supabase_key:
            logger.error("âŒ Supabase ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            raise ValueError("Supabase ì„¤ì • í•„ìš”")
        
        self.supabase = create_client(supabase_url, supabase_key)
        logger.info("âœ… Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        
        self.stats = {
            'start_time': datetime.now().isoformat(),
            'platforms': {},
            'brands': {},
            'total_collected': 0,
            'total_saved': 0,
            'errors': []
        }
    
    def init_driver(self):
        """Selenium ë“œë¼ì´ë²„ ì´ˆê¸°í™”"""
        try:
            options = Options()
            options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument('--window-size=1920,1080')
            options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=options)
            logger.info("âœ… ChromeDriver ì´ˆê¸°í™” ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"âŒ ChromeDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def close_driver(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            self.driver.quit()
            logger.info("ChromeDriver ì¢…ë£Œ")
    
    def crawl_naver_brand(self, brand_name, max_items=10):
        """
        ë„¤ì´ë²„ ì‡¼í•‘ë¼ì´ë¸Œì—ì„œ íŠ¹ì • ë¸Œëœë“œ ë°ì´í„° ìˆ˜ì§‘
        
        Args:
            brand_name (str): ë¸Œëœë“œëª…
            max_items (int): ìµœëŒ€ ìˆ˜ì§‘ ê°œìˆ˜
            
        Returns:
            list: ìˆ˜ì§‘ëœ ë¼ì´ë¸Œ ë°©ì†¡ ë°ì´í„°
        """
        collected_lives = []
        
        try:
            # URL ì¸ì½”ë”©
            encoded_brand = urllib.parse.quote(brand_name)
            search_url = f"https://shoppinglive.naver.com/search/lives?query={encoded_brand}"
            
            logger.info(f"   ğŸ” {brand_name} ê²€ìƒ‰ ì¤‘...")
            
            # í˜ì´ì§€ ë¡œë“œ
            self.driver.get(search_url)
            time.sleep(3)
            
            # ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ê²°ê³¼ ë¡œë“œ
            for i in range(3):
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(1.5)
            
            # HTML íŒŒì‹±
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # ë¼ì´ë¸Œ ë°©ì†¡ ë§í¬ ì¶”ì¶œ
            live_urls = []
            
            # ì„ íƒìë¡œ ë§í¬ ì°¾ê¸°
            for link in soup.select('a[href*="/replays/"], a[href*="/lives/"]'):
                href = link.get('href')
                if href and ('/replays/' in href or '/lives/' in href):
                    # ì ˆëŒ€ URLë¡œ ë³€í™˜
                    if href.startswith('/'):
                        href = f"https://view.shoppinglive.naver.com{href}"
                    
                    # ì¤‘ë³µ ì œê±°
                    if href not in live_urls:
                        live_urls.append(href)
            
            logger.info(f"      {len(live_urls)}ê°œ ë¼ì´ë¸Œ ë°©ì†¡ ë°œê²¬")
            
            # ê° ë¼ì´ë¸Œ ë°©ì†¡ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
            for idx, url in enumerate(live_urls[:max_items], 1):
                    try:
                        logger.info(f"      [{idx}/{min(len(live_urls), max_items)}] ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
                        
                        live_data = self.crawl_live_detail(url, brand_name, 'NAVER', 'ë„¤ì´ë²„')
                        
                        if live_data:
                            collected_lives.append(live_data)
                            title = live_data.get('meta', {}).get('live_title_customer', 'ì œëª© ì—†ìŒ')
                            logger.info(f"         âœ… {title[:30]}...")
                        
                        # ì„œë²„ ë¶€í•˜ ë°©ì§€
                        time.sleep(2)
                        
                    except Exception as e:
                        logger.error(f"         âŒ ìƒì„¸ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                        # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ê¸°ë³¸ ë°ì´í„°ëŠ” ìˆ˜ì§‘
                        try:
                            live_id = url.split('/')[-1].split('?')[0]
                            live_id = f"REAL_NAVER_{brand_name.upper()}_{live_id}"
                            
                            basic_data = {
                                'meta': {
                                    'live_id': live_id,
                                    'platform_name': 'ë„¤ì´ë²„',
                                    'brand_name': brand_name,
                                    'live_title_customer': f"{brand_name} ë¼ì´ë¸Œ ë°©ì†¡",
                                    'live_title_cs': f"{brand_name} {datetime.now().strftime('%Y-%m-%d')} ë¼ì´ë¸Œ",
                                    'source_url': url,
                                    'thumbnail_url': None,
                                    'collected_at': datetime.now().isoformat(),
                                    'status': 'PENDING'
                                },
                                'schedule': {
                                    'broadcast_date': datetime.now().strftime('%Y-%m-%d'),
                                    'broadcast_start_time': '19:00:00',
                                    'broadcast_end_time': '20:00:00',
                                    'benefit_valid_type': 'LIVE_ONLY',
                                    'broadcast_type': 'LIVE'
                                }
                            }
                            collected_lives.append(basic_data)
                            logger.info(f"         âš ï¸ ê¸°ë³¸ ë°ì´í„°ë¡œ ìˆ˜ì§‘")
                        except:
                            pass
                        continue
            
            logger.info(f"   âœ… {brand_name}: {len(collected_lives)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"   âŒ {brand_name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        return collected_lives
    
    def crawl_live_detail(self, url, brand_name, platform_code, platform_name):
        """
        ë¼ì´ë¸Œ ë°©ì†¡ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
        
        Args:
            url (str): ë¼ì´ë¸Œ ë°©ì†¡ URL
            brand_name (str): ë¸Œëœë“œëª…
            platform_code (str): í”Œë«í¼ ì½”ë“œ
            platform_name (str): í”Œë«í¼ëª…
            
        Returns:
            dict: ë¼ì´ë¸Œ ë°©ì†¡ ë°ì´í„°
        """
        try:
            # í˜ì´ì§€ ë¡œë“œ
            self.driver.get(url)
            time.sleep(3)
            
            # HTML íŒŒì‹±
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # live_id ì¶”ì¶œ (URLì—ì„œ)
            live_id = url.split('/')[-1].split('?')[0]
            live_id = f"REAL_{platform_code}_{brand_name.upper()}_{live_id}"
            
            # ì œëª© ì¶”ì¶œ
            title = None
            for selector in ['h1', '.title', '[class*="title"]', 'meta[property="og:title"]']:
                if selector.startswith('meta'):
                    element = soup.select_one(selector)
                    if element:
                        title = element.get('content')
                        break
                else:
                    element = soup.select_one(selector)
                    if element:
                        title = element.get_text(strip=True)
                        if title and title != 'ì‡¼í•‘ë¼ì´ë¸Œ':
                            break
            
            # ê¸°ë³¸ ì œëª© ì„¤ì •
            if not title or title == 'ì‡¼í•‘ë¼ì´ë¸Œ':
                title = f"{brand_name} ë¼ì´ë¸Œ ë°©ì†¡"
            
            # ì¸ë„¤ì¼ ì¶”ì¶œ
            thumbnail_url = None
            meta_image = soup.select_one('meta[property="og:image"]')
            if meta_image:
                thumbnail_url = meta_image.get('content')
            
            # í˜„ì¬ ë‚ ì§œ/ì‹œê°„
            now = datetime.now()
            broadcast_date = now.strftime('%Y-%m-%d')
            
            # ë¼ì´ë¸Œ ë°©ì†¡ ë°ì´í„° êµ¬ì¡°
            live_data = {
                'meta': {
                    'live_id': live_id,
                    'platform_name': platform_name,
                    'brand_name': brand_name,
                    'live_title_customer': title,
                    'live_title_cs': f"{brand_name} {now.strftime('%Y-%m-%d')} ë¼ì´ë¸Œ",
                    'source_url': url,
                    'thumbnail_url': thumbnail_url,
                    'collected_at': now.isoformat(),
                    'status': 'PENDING'
                },
                'schedule': {
                    'broadcast_date': broadcast_date,
                    'broadcast_start_time': '19:00:00',
                    'broadcast_end_time': '20:00:00',
                    'benefit_valid_type': 'LIVE_ONLY',
                    'broadcast_type': 'LIVE'
                },
                'products': [],
                'benefits': {
                    'discounts': [],
                    'gifts': [],
                    'coupons': [],
                    'points': []
                },
                'live_specific': {
                    'key_mentions': [],
                    'broadcast_qa': [],
                    'timeline_summary': []
                },
                'cs_info': {
                    'expected_questions': [],
                    'response_scripts': [],
                    'risk_points': [],
                    'cs_note': f"{brand_name} {platform_name} ë¼ì´ë¸Œ ë°©ì†¡"
                },
                'restrictions': {},
                'duplicate_policy': {}
            }
            
            return live_data
            
        except Exception as e:
            logger.error(f"ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return None
    
    def save_to_supabase(self, live_data):
        """
        Supabaseì— ë°ì´í„° ì €ì¥
        
        Args:
            live_data (dict): ë¼ì´ë¸Œ ë°©ì†¡ ë°ì´í„°
            
        Returns:
            bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            meta = live_data['meta']
            schedule = live_data['schedule']
            
            # ì±„ë„ ID ì¡°íšŒ
            channel_code = self.get_channel_code_from_platform(meta['platform_name'])
            
            response = self.supabase.table('channels').select('channel_id').eq('channel_code', channel_code).execute()
            
            if not response.data or len(response.data) == 0:
                logger.warning(f"   ì±„ë„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {meta['platform_name']} ({channel_code})")
                return False
            
            channel_id = response.data[0]['channel_id']
            
            # ë¼ì´ë¸Œ ë°©ì†¡ ë°ì´í„°
            broadcast_data = {
                'live_id': meta['live_id'],
                'channel_id': channel_id,
                'channel_code': self.get_channel_code_from_platform(meta['platform_name']),
                'platform_name': meta['platform_name'],
                'brand_name': meta['brand_name'],
                'live_title_customer': meta['live_title_customer'],
                'live_title_cs': meta['live_title_cs'],
                'source_url': meta['source_url'],
                'thumbnail_url': meta.get('thumbnail_url'),
                'broadcast_date': schedule['broadcast_date'],
                'broadcast_start_time': schedule.get('broadcast_start_time'),
                'broadcast_end_time': schedule.get('broadcast_end_time'),
                'benefit_valid_type': schedule.get('benefit_valid_type'),
                'broadcast_type': schedule.get('broadcast_type'),
                'status': meta.get('status', 'PENDING'),
                'collected_at': meta['collected_at']
            }
            
            # Supabaseì— ì €ì¥ (UPSERT)
            response = self.supabase.table('live_broadcasts').upsert(
                broadcast_data,
                on_conflict='live_id'
            ).execute()
            
            if response.data:
                logger.info(f"      âœ… Supabase ì €ì¥ ì™„ë£Œ: {meta['live_id']}")
                return True
            else:
                logger.warning(f"      âš ï¸ Supabase ì €ì¥ ì‹¤íŒ¨: {meta['live_id']}")
                return False
                
        except Exception as e:
            logger.error(f"   Supabase ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def get_channel_code_from_platform(self, platform_name):
        """í”Œë«í¼ ì´ë¦„ì„ ì±„ë„ ì½”ë“œë¡œ ë³€í™˜"""
        mapping = {
            'ë„¤ì´ë²„': 'NAVER',
            'ì¹´ì¹´ì˜¤': 'KAKAO',
            '11ë²ˆê°€': '11ST',
            'Gë§ˆì¼“': 'GMARKET',
            'ì˜¬ë¦¬ë¸Œì˜': 'OLIVEYOUNG',
            'ê·¸ë¦½': 'GRIP',
            'ë¬´ì‹ ì‚¬': 'MUSINSA',
            'ë¡¯ë°ì˜¨': 'LOTTEON',
            'ì•„ëª¨ë ˆëª°': 'AMOREMALL',
            'ì´ë‹ˆìŠ¤í”„ë¦¬ëª°': 'INNISFREE_MALL'
        }
        return mapping.get(platform_name, 'NAVER')
    
    def crawl_all_platforms_and_brands(self):
        """ëª¨ë“  í”Œë«í¼ê³¼ ë¸Œëœë“œ ë°ì´í„° ìˆ˜ì§‘"""
        
        logger.info("=" * 80)
        logger.info("ğŸš€ ë©€í‹° í”Œë«í¼ Ã— ë©€í‹° ë¸Œëœë“œ ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        logger.info("=" * 80)
        logger.info(f"ìˆ˜ì§‘ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"ëŒ€ìƒ í”Œë«í¼: {len([p for p in PLATFORMS if p.get('isActive', True)])}ê°œ")
        logger.info(f"ëŒ€ìƒ ë¸Œëœë“œ: {len(BRANDS)}ê°œ")
        logger.info("=" * 80)
        
        # ë“œë¼ì´ë²„ ì´ˆê¸°í™”
        if not self.init_driver():
            logger.error("âŒ ë“œë¼ì´ë²„ ì´ˆê¸°í™” ì‹¤íŒ¨")
            return
        
        try:
            # í˜„ì¬ëŠ” ë„¤ì´ë²„ í”Œë«í¼ë§Œ êµ¬í˜„ (ë‹¤ë¥¸ í”Œë«í¼ì€ ì¶”í›„ í™•ì¥)
            active_platforms = [p for p in PLATFORMS if p.get('isActive', True) and p['code'] == 'NAVER']
            
            for platform in active_platforms:
                platform_code = platform['code']
                platform_name = platform['name']
                
                logger.info(f"\n{'='*80}")
                logger.info(f"ğŸ“¦ í”Œë«í¼: {platform_name} ({platform_code})")
                logger.info(f"{'='*80}")
                
                self.stats['platforms'][platform_code] = {
                    'name': platform_name,
                    'brands_processed': 0,
                    'lives_collected': 0,
                    'lives_saved': 0
                }
                
                # ê° ë¸Œëœë“œë³„ë¡œ ìˆ˜ì§‘
                for idx, brand in enumerate(BRANDS, 1):
                    brand_name = brand['name']
                    
                    logger.info(f"\n[{idx}/{len(BRANDS)}] {brand_name} ë¸Œëœë“œ ì²˜ë¦¬ ì¤‘...")
                    
                    try:
                            # ë„¤ì´ë²„ì—ì„œ ë¸Œëœë“œ ë°ì´í„° ìˆ˜ì§‘
                        lives = self.crawl_naver_brand(brand_name, max_items=10)
                        
                        # Supabaseì— ì €ì¥
                        saved_count = 0
                        for live_data in lives:
                            try:
                                if self.save_to_supabase(live_data):
                                    saved_count += 1
                            except Exception as save_error:
                                logger.error(f"      ì €ì¥ ì¤‘ ì˜¤ë¥˜: {save_error}")
                        
                        # í†µê³„ ì—…ë°ì´íŠ¸
                        self.stats['platforms'][platform_code]['brands_processed'] += 1
                        self.stats['platforms'][platform_code]['lives_collected'] += len(lives)
                        self.stats['platforms'][platform_code]['lives_saved'] += saved_count
                        self.stats['total_collected'] += len(lives)
                        self.stats['total_saved'] += saved_count
                        
                        if brand_name not in self.stats['brands']:
                            self.stats['brands'][brand_name] = 0
                        self.stats['brands'][brand_name] += len(lives)
                        
                        logger.info(f"   âœ… {brand_name}: {len(lives)}ê°œ ìˆ˜ì§‘, {saved_count}ê°œ ì €ì¥")
                        
                    except Exception as e:
                        logger.error(f"   âŒ {brand_name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        self.stats['errors'].append({
                            'platform': platform_name,
                            'brand': brand_name,
                            'error': str(e)
                        })
                    
                    # ë¸Œëœë“œ ê°„ ë”œë ˆì´
                    time.sleep(3)
            
            # ìµœì¢… í†µê³„
            self.print_final_stats()
            
        except Exception as e:
            logger.error(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        finally:
            self.close_driver()
            self.save_stats()
    
    def print_final_stats(self):
        """ìµœì¢… í†µê³„ ì¶œë ¥"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        logger.info("=" * 80)
        logger.info(f"ìˆ˜ì§‘ ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"ì´ ìˆ˜ì§‘: {self.stats['total_collected']}ê°œ")
        logger.info(f"ì´ ì €ì¥: {self.stats['total_saved']}ê°œ")
        
        logger.info(f"\nğŸ“Š í”Œë«í¼ë³„ í†µê³„:")
        for code, stats in self.stats['platforms'].items():
            logger.info(f"  {stats['name']}:")
            logger.info(f"    - ì²˜ë¦¬ ë¸Œëœë“œ: {stats['brands_processed']}ê°œ")
            logger.info(f"    - ìˆ˜ì§‘: {stats['lives_collected']}ê°œ")
            logger.info(f"    - ì €ì¥: {stats['lives_saved']}ê°œ")
        
        logger.info(f"\nğŸ“Š ë¸Œëœë“œë³„ í†µê³„:")
        for brand, count in sorted(self.stats['brands'].items(), key=lambda x: x[1], reverse=True):
            logger.info(f"  - {brand}: {count}ê°œ")
        
        if self.stats['errors']:
            logger.info(f"\nâš ï¸ ì—ëŸ¬ ë°œìƒ: {len(self.stats['errors'])}ê±´")
            for error in self.stats['errors'][:5]:
                logger.info(f"  - {error['platform']}/{error['brand']}: {error['error'][:50]}")
        
        logger.info("=" * 80)
    
    def save_stats(self):
        """í†µê³„ ì €ì¥"""
        try:
            output_dir = Path(__file__).parent / 'output'
            output_dir.mkdir(exist_ok=True)
            
            stats_file = output_dir / f'crawl_stats_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
            
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(self.stats, f, ensure_ascii=False, indent=2)
            
            logger.info(f"í†µê³„ ì €ì¥ ì™„ë£Œ: {stats_file}")
            
        except Exception as e:
            logger.error(f"í†µê³„ ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ì‹¤ì œ í”Œë«í¼ ë°ì´í„° ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    logger.info("ì´ ì‘ì—…ì€ ì•½ 10-20ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")
    
    crawler = ImprovedMultiPlatformCrawler()
    crawler.crawl_all_platforms_and_brands()
    
    logger.info("\nâœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")


if __name__ == "__main__":
    main()
