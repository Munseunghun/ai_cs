#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ÎÑ§Ïù¥Î≤Ñ ÏáºÌïëÎùºÏù¥Î∏å Ï¢ÖÌï© ÌÅ¨Î°§Îü¨
ÏÉÅÌíà, Ïø†Ìè∞, ÌòúÌÉù, ÎåìÍ∏Ä, Ï±ÑÌåÖ, FAQ, ÎùºÏù¥Î∏å ÏÜåÍ∞ú Îì± Î™®Îì† Ï†ïÎ≥¥ ÏàòÏßë
"""

import sys
import time
import json
import logging
import re
from datetime import datetime, timedelta
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

# Î°úÏª¨ Î™®Îìà
sys.path.append('.')
from supabase import create_client
from dotenv import load_dotenv
import os

# ÌôòÍ≤ΩÎ≥ÄÏàò Î°úÎìú
load_dotenv()

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ComprehensiveNaverCrawler:
    """ÎÑ§Ïù¥Î≤Ñ ÏáºÌïëÎùºÏù¥Î∏å Ï¢ÖÌï© ÌÅ¨Î°§Îü¨"""
    
    def __init__(self):
        """ÌÅ¨Î°§Îü¨ Ï¥àÍ∏∞Ìôî"""
        self.driver = None
        
        # Supabase ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî
        supabase_url = os.getenv('SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_ANON_KEY')
        
        if not supabase_url or not supabase_key:
            logger.error("‚ùå Supabase ÏÑ§Ï†ïÏù¥ ÏóÜÏäµÎãàÎã§.")
            raise ValueError("Supabase ÏÑ§Ï†ï ÌïÑÏöî")
        
        self.supabase = create_client(supabase_url, supabase_key)
        logger.info("‚úÖ Supabase ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
        
        self.stats = {
            'total_processed': 0,
            'products_collected': 0,
            'coupons_collected': 0,
            'comments_collected': 0,
            'faqs_collected': 0,
            'errors': []
        }
    
    def init_driver(self):
        """Selenium ÎìúÎùºÏù¥Î≤Ñ Ï¥àÍ∏∞Ìôî"""
        try:
            options = Options()
            options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument('--window-size=1920,1080')
            options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')
            
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=options)
            logger.info("‚úÖ ChromeDriver Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
            return True
        except Exception as e:
            logger.error(f"‚ùå ChromeDriver Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            return False
    
    def close_driver(self):
        """ÎìúÎùºÏù¥Î≤Ñ Ï¢ÖÎ£å"""
        if self.driver:
            self.driver.quit()
            logger.info("ChromeDriver Ï¢ÖÎ£å")
    
    def crawl_comprehensive_data(self, p_live_url, p_live_id, p_brand_name=''):
        """
        ÎùºÏù¥Î∏å Î∞©ÏÜ°Ïùò Î™®Îì† Ï†ïÎ≥¥ ÏàòÏßë
        
        Args:
            p_live_url (str): ÎùºÏù¥Î∏å Î∞©ÏÜ° URL
            p_live_id (str): ÎùºÏù¥Î∏å Î∞©ÏÜ° ID
            p_brand_name (str): Î∏åÎûúÎìúÎ™Ö
            
        Returns:
            dict: ÏàòÏßëÎêú Î™®Îì† Ï†ïÎ≥¥
        """
        try:
            logger.info(f"   üé¨ Ï¢ÖÌï© Ï†ïÎ≥¥ ÏàòÏßë Ï§ë: {p_live_id}")
            
            # ÌéòÏù¥ÏßÄ Î°úÎìú
            self.driver.get(p_live_url)
            time.sleep(8)  # ÌéòÏù¥ÏßÄ Î°úÎìú ÎåÄÍ∏∞
            
            # Ïä§ÌÅ¨Î°§ÌïòÏó¨ Î™®Îì† ÏΩòÌÖêÏ∏† Î°úÎìú
            for i in range(5):
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(2)
            
            # ÏÉÅÎã®ÏúºÎ°ú Ïä§ÌÅ¨Î°§
            self.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(2)
            
            # HTML ÌååÏã±
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            comprehensive_data = {
                'live_id': p_live_id,
                'brand_name': p_brand_name,
                'products': [],
                'coupons': [],
                'comments': [],
                'faqs': [],
                'intro': {},
                'statistics': {},
                'images': []
            }
            
            # 1. ÏÉÅÌíà Ï†ïÎ≥¥ ÏàòÏßë
            products = self._extract_products(soup, p_live_id)
            if products:
                comprehensive_data['products'] = products
                logger.info(f"      ‚úÖ ÏÉÅÌíà: {len(products)}Í∞ú")
            
            # 2. Ïø†Ìè∞ Ï†ïÎ≥¥ ÏàòÏßë
            coupons = self._extract_coupons(soup, p_live_id)
            if coupons:
                comprehensive_data['coupons'] = coupons
                logger.info(f"      ‚úÖ Ïø†Ìè∞: {len(coupons)}Í∞ú")
            
            # 3. ÎåìÍ∏Ä/Ï±ÑÌåÖ ÏàòÏßë
            comments = self._extract_comments(soup, p_live_id)
            if comments:
                comprehensive_data['comments'] = comments
                logger.info(f"      ‚úÖ ÎåìÍ∏Ä: {len(comments)}Í∞ú")
            
            # 4. FAQ ÏàòÏßë (ÎåìÍ∏ÄÏóêÏÑú ÏßàÎ¨∏ Ìå®ÌÑ¥ Ï∂îÏ∂ú)
            faqs = self._extract_faqs(comments, p_live_id)
            if faqs:
                comprehensive_data['faqs'] = faqs
                logger.info(f"      ‚úÖ FAQ: {len(faqs)}Í∞ú")
            
            # 5. ÎùºÏù¥Î∏å ÏÜåÍ∞ú ÏàòÏßë
            intro = self._extract_intro(soup, p_live_id, p_brand_name)
            if intro:
                comprehensive_data['intro'] = intro
                logger.info(f"      ‚úÖ ÎùºÏù¥Î∏å ÏÜåÍ∞ú ÏàòÏßë ÏôÑÎ£å")
            
            # 6. ÌÜµÍ≥Ñ Ï†ïÎ≥¥ ÏàòÏßë
            statistics = self._extract_statistics(soup, p_live_id)
            if statistics:
                comprehensive_data['statistics'] = statistics
                logger.info(f"      ‚úÖ ÌÜµÍ≥Ñ Ï†ïÎ≥¥ ÏàòÏßë ÏôÑÎ£å")
            
            # 7. Ïù¥ÎØ∏ÏßÄ ÏàòÏßë
            images = self._extract_images(soup, p_live_id)
            if images:
                comprehensive_data['images'] = images
                logger.info(f"      ‚úÖ Ïù¥ÎØ∏ÏßÄ: {len(images)}Í∞ú")
            
            return comprehensive_data
            
        except Exception as e:
            logger.error(f"   ‚ùå Ï¢ÖÌï© Ï†ïÎ≥¥ ÏàòÏßë Ïã§Ìå®: {e}")
            return None
    
    def _extract_products(self, p_soup, p_live_id):
        """ÏÉÅÌíà Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
        products = []
        
        try:
            # ÎÑ§Ïù¥Î≤Ñ ÏáºÌïëÎùºÏù¥Î∏å ÏÉÅÌíà ÏÑ†ÌÉùÏûê
            product_elements = p_soup.select('[class*="ProductWrapper"], [class*="ProductCard"]')
            
            for idx, elem in enumerate(product_elements[:50], 1):  # ÏµúÎåÄ 50Í∞ú
                try:
                    # ÏÉÅÌíàÎ™Ö
                    name_elem = elem.select_one('[class*="ProductName"]')
                    product_name = name_elem.get_text(strip=True) if name_elem else None
                    
                    if not product_name:
                        continue
                    
                    # Í∞ÄÍ≤© Ï†ïÎ≥¥
                    price_elem = elem.select_one('[class*="ProductPrice"]')
                    sale_price = None
                    original_price = None
                    discount_rate = None
                    
                    if price_elem:
                        price_text = price_elem.get_text(strip=True)
                        # Ìï†Ïù∏Ïú® Ï∂îÏ∂ú
                        discount_match = re.search(r'(\d+)%', price_text)
                        if discount_match:
                            discount_rate = int(discount_match.group(1))
                        
                        # Í∞ÄÍ≤© Ï∂îÏ∂ú
                        price_numbers = re.findall(r'([\d,]+)Ïõê', price_text)
                        if len(price_numbers) >= 2:
                            sale_price = int(price_numbers[1].replace(',', ''))
                            original_price = int(price_numbers[0].replace(',', ''))
                        elif len(price_numbers) == 1:
                            sale_price = int(price_numbers[0].replace(',', ''))
                    
                    # Ïù¥ÎØ∏ÏßÄ
                    img_elem = elem.select_one('img')
                    product_image = img_elem.get('src', '') if img_elem else None
                    
                    # ÎßÅÌÅ¨
                    link_elem = elem.select_one('a')
                    product_link = link_elem.get('href', '') if link_elem else None
                    
                    # Î™∞ Ïù¥Î¶Ñ
                    mall_elem = elem.select_one('[class*="MallName"]')
                    mall_name = mall_elem.get_text(strip=True) if mall_elem else None
                    
                    # Î∞∞ÏÜ°ÎπÑ
                    delivery_elem = elem.select_one('[class*="delivery"]')
                    delivery_fee = delivery_elem.get_text(strip=True) if delivery_elem else None
                    is_free_delivery = 'Î¨¥Î£åÎ∞∞ÏÜ°' in (delivery_fee or '')
                    
                    product = {
                        'live_id': p_live_id,
                        'product_name': product_name,
                        'sale_price': sale_price,
                        'original_price': original_price,
                        'discount_rate': discount_rate,
                        'product_image_url': product_image,
                        'product_link': product_link,
                        'mall_name': mall_name,
                        'delivery_fee': delivery_fee,
                        'is_free_delivery': is_free_delivery
                    }
                    
                    products.append(product)
                    
                except Exception as e:
                    logger.warning(f"      ÏÉÅÌíà {idx} ÌååÏã± Ïã§Ìå®: {e}")
                    continue
        
        except Exception as e:
            logger.warning(f"      ÏÉÅÌíà Ï∂îÏ∂ú Ïã§Ìå®: {e}")
        
        return products
    
    def _extract_coupons(self, p_soup, p_live_id):
        """Ïø†Ìè∞ Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
        coupons = []
        
        try:
            # Ïø†Ìè∞ ÏÑ†ÌÉùÏûê
            coupon_elements = p_soup.select('[class*="Coupon"], [class*="coupon"]')
            
            for idx, elem in enumerate(coupon_elements, 1):
                try:
                    coupon_text = elem.get_text(strip=True)
                    
                    if not coupon_text or len(coupon_text) < 3:
                        continue
                    
                    # Ìï†Ïù∏Ïú® Ï∂îÏ∂ú
                    discount_rate = None
                    discount_amount = None
                    discount_match = re.search(r'(\d+)%', coupon_text)
                    if discount_match:
                        discount_rate = int(discount_match.group(1))
                    
                    # Ìï†Ïù∏ Í∏àÏï° Ï∂îÏ∂ú
                    amount_match = re.search(r'([\d,]+)Ïõê', coupon_text)
                    if amount_match:
                        discount_amount = int(amount_match.group(1).replace(',', ''))
                    
                    # Ïø†Ìè∞ ÌÉÄÏûÖ ÌåêÎã®
                    coupon_type = 'Ìï†Ïù∏Ïø†Ìè∞'
                    if 'Î¨¥Î£åÎ∞∞ÏÜ°' in coupon_text:
                        coupon_type = 'Î¨¥Î£åÎ∞∞ÏÜ°'
                    elif 'Ï†ÅÎ¶Ω' in coupon_text:
                        coupon_type = 'Ï†ÅÎ¶ΩÏø†Ìè∞'
                    
                    coupon = {
                        'live_id': p_live_id,
                        'coupon_name': coupon_text,
                        'coupon_type': coupon_type,
                        'discount_rate': discount_rate,
                        'discount_amount': discount_amount,
                        'is_active': True
                    }
                    
                    coupons.append(coupon)
                    
                except Exception as e:
                    logger.warning(f"      Ïø†Ìè∞ {idx} ÌååÏã± Ïã§Ìå®: {e}")
                    continue
        
        except Exception as e:
            logger.warning(f"      Ïø†Ìè∞ Ï∂îÏ∂ú Ïã§Ìå®: {e}")
        
        return coupons
    
    def _extract_comments(self, p_soup, p_live_id):
        """ÎåìÍ∏Ä/Ï±ÑÌåÖ Ï∂îÏ∂ú"""
        comments = []
        
        try:
            # ÎåìÍ∏Ä ÏÑ†ÌÉùÏûê
            comment_elements = p_soup.select('[class*="comment"], [class*="Comment"], [class*="chat"]')
            
            for idx, elem in enumerate(comment_elements[:100], 1):  # ÏµúÎåÄ 100Í∞ú
                try:
                    comment_text = elem.get_text(strip=True)
                    
                    if not comment_text or len(comment_text) < 3:
                        continue
                    
                    # ÎåìÍ∏Ä ÌÉÄÏûÖ ÌåêÎã®
                    comment_type = 'comment'
                    if '?' in comment_text or 'Ïñ¥ÎñªÍ≤å' in comment_text or 'Ïñ∏Ï†ú' in comment_text:
                        comment_type = 'question'
                    
                    comment = {
                        'live_id': p_live_id,
                        'comment_text': comment_text,
                        'comment_type': comment_type,
                        'like_count': 0,
                        'reply_count': 0
                    }
                    
                    comments.append(comment)
                    
                except Exception as e:
                    logger.warning(f"      ÎåìÍ∏Ä {idx} ÌååÏã± Ïã§Ìå®: {e}")
                    continue
        
        except Exception as e:
            logger.warning(f"      ÎåìÍ∏Ä Ï∂îÏ∂ú Ïã§Ìå®: {e}")
        
        return comments
    
    def _extract_faqs(self, p_comments, p_live_id):
        """FAQ ÏÉùÏÑ± (ÎåìÍ∏ÄÏóêÏÑú ÏßàÎ¨∏ Ï∂îÏ∂ú + Í∏∞Î≥∏ FAQ)"""
        faqs = []
        
        # ÎåìÍ∏ÄÏóêÏÑú ÏßàÎ¨∏ Ï∂îÏ∂ú
        questions = [c for c in p_comments if c.get('comment_type') == 'question']
        
        for idx, q in enumerate(questions[:10], 1):  # ÏµúÎåÄ 10Í∞ú
            faq = {
                'live_id': p_live_id,
                'question': q['comment_text'],
                'answer': 'ÏÉÅÎã¥ÏõêÏù¥ ÎãµÎ≥Ä ÎìúÎ¶¨Í≤†ÏäµÎãàÎã§.',
                'category': 'Í≥†Í∞ù ÏßàÎ¨∏'
            }
            faqs.append(faq)
        
        # Í∏∞Î≥∏ FAQ Ï∂îÍ∞Ä
        default_faqs = [
            {
                'live_id': p_live_id,
                'question': 'Î∞∞ÏÜ°ÏùÄ Ïñ∏Ï†ú ÎêòÎÇòÏöî?',
                'answer': 'Ï£ºÎ¨∏ ÌõÑ 2-3Ïùº ÎÇ¥ Î∞∞ÏÜ°Îê©ÎãàÎã§.',
                'category': 'Î∞∞ÏÜ°'
            },
            {
                'live_id': p_live_id,
                'question': 'Ïø†Ìè∞ÏùÄ Ïñ¥ÎñªÍ≤å ÏÇ¨Ïö©ÌïòÎÇòÏöî?',
                'answer': 'Î∞©ÏÜ° Ï§ë Ï†úÍ≥µÎêòÎäî Ïø†Ìè∞ Î≤àÌò∏Î•º ÏûÖÎ†•ÌïòÏãúÎ©¥ ÏûêÎèô Ï†ÅÏö©Îê©ÎãàÎã§.',
                'category': 'ÌòúÌÉù'
            },
            {
                'live_id': p_live_id,
                'question': 'Î∞òÌíà/ÍµêÌôòÏùÄ Ïñ¥ÎñªÍ≤å ÌïòÎÇòÏöî?',
                'answer': 'ÏàòÎ†π ÌõÑ 7Ïùº Ïù¥ÎÇ¥ Î∞òÌíà/ÍµêÌôò Í∞ÄÎä•ÌïòÎ©∞, Í≥†Í∞ùÏÑºÌÑ∞Î°ú Î¨∏ÏùòÏ£ºÏãúÎ©¥ ÏïàÎÇ¥Ìï¥ÎìúÎ¶ΩÎãàÎã§.',
                'category': 'Î∞òÌíà/ÍµêÌôò'
            }
        ]
        
        faqs.extend(default_faqs)
        
        return faqs
    
    def _extract_intro(self, p_soup, p_live_id, p_brand_name):
        """ÎùºÏù¥Î∏å ÏÜåÍ∞ú Ï∂îÏ∂ú"""
        intro = {
            'live_id': p_live_id
        }
        
        try:
            # Ï†úÎ™©
            title = p_soup.find('meta', property='og:title')
            if title:
                intro['intro_title'] = title.get('content', '')
            
            # ÏÑ§Î™Ö
            desc = p_soup.find('meta', property='og:description')
            if desc:
                intro['intro_description'] = desc.get('content', '')
            
            # Î∏åÎûúÎìúÎ™Ö
            intro['host_name'] = p_brand_name
            
            # ÌïòÏù¥ÎùºÏù¥Ìä∏ (Ï†úÎ™©ÏóêÏÑú Ï∂îÏ∂ú)
            if intro.get('intro_title'):
                highlights = []
                if 'Ìï†Ïù∏' in intro['intro_title']:
                    highlights.append('ÌäπÎ≥Ñ Ìï†Ïù∏')
                if 'Ïã†ÏÉÅ' in intro['intro_title'] or 'Î°†Ïπ≠' in intro['intro_title']:
                    highlights.append('Ïã†ÏÉÅÌíà Ï∂úÏãú')
                if '%' in intro['intro_title']:
                    highlights.append('ÌååÍ≤© Ìï†Ïù∏')
                
                intro['intro_highlights'] = json.dumps(highlights, ensure_ascii=False)
        
        except Exception as e:
            logger.warning(f"      ÎùºÏù¥Î∏å ÏÜåÍ∞ú Ï∂îÏ∂ú Ïã§Ìå®: {e}")
        
        return intro
    
    def _extract_statistics(self, p_soup, p_live_id):
        """ÌÜµÍ≥Ñ Ï†ïÎ≥¥ Ï∂îÏ∂ú"""
        statistics = {
            'live_id': p_live_id,
            'view_count': 0,
            'like_count': 0,
            'comment_count': 0
        }
        
        try:
            # Ï¢ãÏïÑÏöî Ïàò
            like_elem = p_soup.select_one('[class*="Like"], [class*="like"]')
            if like_elem:
                like_text = like_elem.get_text(strip=True)
                like_match = re.search(r'(\d+)', like_text)
                if like_match:
                    statistics['like_count'] = int(like_match.group(1))
        
        except Exception as e:
            logger.warning(f"      ÌÜµÍ≥Ñ Ï∂îÏ∂ú Ïã§Ìå®: {e}")
        
        return statistics
    
    def _extract_images(self, p_soup, p_live_id):
        """Ïù¥ÎØ∏ÏßÄ Ï∂îÏ∂ú"""
        images = []
        
        try:
            # Ïç∏ÎÑ§Ïùº
            og_image = p_soup.find('meta', property='og:image')
            if og_image:
                images.append({
                    'live_id': p_live_id,
                    'image_url': og_image.get('content', ''),
                    'image_type': 'thumbnail'
                })
            
            # Ï†úÌíà Ïù¥ÎØ∏ÏßÄ (ÏÉÅÏúÑ 5Í∞ú)
            product_images = p_soup.select('img[src*="phinf"]')
            for idx, img in enumerate(product_images[:5], 2):
                images.append({
                    'live_id': p_live_id,
                    'image_url': img.get('src', ''),
                    'image_type': 'product',
                    'image_alt': img.get('alt', '')
                })
        
        except Exception as e:
            logger.warning(f"      Ïù¥ÎØ∏ÏßÄ Ï∂îÏ∂ú Ïã§Ìå®: {e}")
        
        return images
    
    def save_comprehensive_data(self, p_data):
        """
        ÏàòÏßëÎêú Î™®Îì† Îç∞Ïù¥ÌÑ∞Î•º SupabaseÏóê Ï†ÄÏû•
        
        Args:
            p_data (dict): ÏàòÏßëÎêú Ï¢ÖÌï© Îç∞Ïù¥ÌÑ∞
            
        Returns:
            bool: Ï†ÄÏû• ÏÑ±Í≥µ Ïó¨Î∂Ä
        """
        try:
            live_id = p_data['live_id']
            logger.info(f"   üíæ Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• Ï§ë: {live_id}")
            
            # 1. ÏÉÅÌíà Ï†ÄÏû•
            if p_data.get('products'):
                for product in p_data['products']:
                    try:
                        self.supabase.table('live_products').upsert(product).execute()
                    except Exception as e:
                        logger.warning(f"      ÏÉÅÌíà Ï†ÄÏû• Ïã§Ìå®: {e}")
                self.stats['products_collected'] += len(p_data['products'])
                logger.info(f"      ‚úÖ ÏÉÅÌíà {len(p_data['products'])}Í∞ú Ï†ÄÏû•")
            
            # 2. Ïø†Ìè∞ Ï†ÄÏû•
            if p_data.get('coupons'):
                for coupon in p_data['coupons']:
                    try:
                        self.supabase.table('live_coupons').insert(coupon).execute()
                    except Exception as e:
                        logger.warning(f"      Ïø†Ìè∞ Ï†ÄÏû• Ïã§Ìå®: {e}")
                self.stats['coupons_collected'] += len(p_data['coupons'])
                logger.info(f"      ‚úÖ Ïø†Ìè∞ {len(p_data['coupons'])}Í∞ú Ï†ÄÏû•")
            
            # 3. ÎåìÍ∏Ä Ï†ÄÏû•
            if p_data.get('comments'):
                for comment in p_data['comments']:
                    try:
                        self.supabase.table('live_comments').insert(comment).execute()
                    except Exception as e:
                        logger.warning(f"      ÎåìÍ∏Ä Ï†ÄÏû• Ïã§Ìå®: {e}")
                self.stats['comments_collected'] += len(p_data['comments'])
                logger.info(f"      ‚úÖ ÎåìÍ∏Ä {len(p_data['comments'])}Í∞ú Ï†ÄÏû•")
            
            # 4. FAQ Ï†ÄÏû•
            if p_data.get('faqs'):
                for faq in p_data['faqs']:
                    try:
                        self.supabase.table('live_faqs').insert(faq).execute()
                    except Exception as e:
                        logger.warning(f"      FAQ Ï†ÄÏû• Ïã§Ìå®: {e}")
                self.stats['faqs_collected'] += len(p_data['faqs'])
                logger.info(f"      ‚úÖ FAQ {len(p_data['faqs'])}Í∞ú Ï†ÄÏû•")
            
            # 5. ÎùºÏù¥Î∏å ÏÜåÍ∞ú Ï†ÄÏû•
            if p_data.get('intro') and p_data['intro'].get('intro_title'):
                try:
                    self.supabase.table('live_intro').upsert(
                        p_data['intro'],
                        on_conflict='live_id'
                    ).execute()
                    logger.info(f"      ‚úÖ ÎùºÏù¥Î∏å ÏÜåÍ∞ú Ï†ÄÏû•")
                except Exception as e:
                    logger.warning(f"      ÎùºÏù¥Î∏å ÏÜåÍ∞ú Ï†ÄÏû• Ïã§Ìå®: {e}")
            
            # 6. ÌÜµÍ≥Ñ Ï†ÄÏû•
            if p_data.get('statistics'):
                try:
                    self.supabase.table('live_statistics').insert(p_data['statistics']).execute()
                    logger.info(f"      ‚úÖ ÌÜµÍ≥Ñ Ï†ïÎ≥¥ Ï†ÄÏû•")
                except Exception as e:
                    logger.warning(f"      ÌÜµÍ≥Ñ Ï†ÄÏû• Ïã§Ìå®: {e}")
            
            # 7. Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû•
            if p_data.get('images'):
                for image in p_data['images']:
                    try:
                        self.supabase.table('live_images').insert(image).execute()
                    except Exception as e:
                        logger.warning(f"      Ïù¥ÎØ∏ÏßÄ Ï†ÄÏû• Ïã§Ìå®: {e}")
                logger.info(f"      ‚úÖ Ïù¥ÎØ∏ÏßÄ {len(p_data['images'])}Í∞ú Ï†ÄÏû•")
            
            return True
            
        except Exception as e:
            logger.error(f"   ‚ùå Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• Ï§ë ÏóêÎü¨: {e}")
            return False
    
    def crawl_live(self, p_live_url, p_live_id, p_brand_name=''):
        """ÎùºÏù¥Î∏å Î∞©ÏÜ° 1Í∞ú ÌÅ¨Î°§ÎßÅ Î∞è Ï†ÄÏû•"""
        try:
            # Ï¢ÖÌï© Îç∞Ïù¥ÌÑ∞ ÏàòÏßë
            data = self.crawl_comprehensive_data(p_live_url, p_live_id, p_brand_name)
            
            if data:
                # Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•
                if self.save_comprehensive_data(data):
                    self.stats['total_processed'] += 1
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"   ‚ùå ÌÅ¨Î°§ÎßÅ Ïã§Ìå®: {e}")
            self.stats['errors'].append({
                'live_id': p_live_id,
                'error': str(e)
            })
            return False


def main():
    """Î©îÏù∏ Ìï®Ïàò"""
    try:
        crawler = ComprehensiveNaverCrawler()
        
        # ÎìúÎùºÏù¥Î≤Ñ Ï¥àÍ∏∞Ìôî
        if not crawler.init_driver():
            logger.error("‚ùå ÎìúÎùºÏù¥Î≤Ñ Ï¥àÍ∏∞Ìôî Ïã§Ìå®")
            return 1
        
        # ÌÖåÏä§Ìä∏: ÏÉòÌîå URL ÌÅ¨Î°§ÎßÅ
        test_url = "https://view.shoppinglive.naver.com/replays/1744150?fm=shoppinglive&sn=home&tr=lim"
        test_live_id = "REAL_NAVER_ÎùºÎÑ§Ï¶à_1744150"
        test_brand = "ÎùºÎÑ§Ï¶à"
        
        logger.info("üéØ Ï¢ÖÌï© ÌÅ¨Î°§ÎßÅ ÏãúÏûë")
        logger.info(f"   URL: {test_url}")
        logger.info(f"   Live ID: {test_live_id}")
        logger.info(f"   Brand: {test_brand}")
        logger.info("=" * 80)
        
        # ÌÅ¨Î°§ÎßÅ Ïã§Ìñâ
        success = crawler.crawl_live(test_url, test_live_id, test_brand)
        
        # ÎìúÎùºÏù¥Î≤Ñ Ï¢ÖÎ£å
        crawler.close_driver()
        
        # ÏµúÏ¢Ö ÌÜµÍ≥Ñ
        logger.info("=" * 80)
        logger.info("üéâ ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å!")
        logger.info(f"   - Ï≤òÎ¶¨ ÏÑ±Í≥µ: {crawler.stats['total_processed']}Í∞ú")
        logger.info(f"   - ÏÉÅÌíà ÏàòÏßë: {crawler.stats['products_collected']}Í∞ú")
        logger.info(f"   - Ïø†Ìè∞ ÏàòÏßë: {crawler.stats['coupons_collected']}Í∞ú")
        logger.info(f"   - ÎåìÍ∏Ä ÏàòÏßë: {crawler.stats['comments_collected']}Í∞ú")
        logger.info(f"   - FAQ ÏàòÏßë: {crawler.stats['faqs_collected']}Í∞ú")
        logger.info(f"   - ÏóêÎü¨: {len(crawler.stats['errors'])}Í∞ú")
        logger.info("=" * 80)
        
        return 0 if success else 1
        
    except Exception as e:
        logger.error(f"‚ùå ÌîÑÎ°úÍ∑∏Îû® Ïã§Ìñâ Ïã§Ìå®: {e}")
        return 1


if __name__ == '__main__':
    sys.exit(main())
