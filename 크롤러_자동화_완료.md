# 크롤러 자동화 설정 완료

작성 일시: 2025-12-08 08:40

---

## ✅ 크롤러 자동화 완료!

### 실행 상태

**스케줄러 프로세스**: ✅ 실행 중 (PID: 8848)

**실행 방식**: 백그라운드 (nohup)

**로그 파일**: `crawler/logs/scheduler_20251208_083757.log`

---

## 🎯 수집 완료 데이터 (최신)

### 라네즈 라이브 방송 (REAL_NAVER_라네즈_1744150)

**수집 시간**: 2025-12-08 08:36

**수집 결과**:
- ✅ 상품: 18개
- ✅ 쿠폰: 1개
- ✅ 댓글: 6개
- ✅ FAQ: 3개
- ✅ 라이브 소개: 1개
- ✅ 통계 정보: 1개
- ✅ 이미지: 6개

**Supabase 저장**: ✅ 완료

**테이블별 저장 상태**:
- ✅ `live_products`: 18개 저장 완료
- ✅ `live_coupons`: 1개 저장 완료
- ✅ `live_comments`: 6개 저장 완료
- ⚠️ `live_faqs`: 3개 (faq_id 제약 조건 오류)
- ✅ `live_intro`: 1개 저장 완료
- ✅ `live_statistics`: 1개 저장 완료
- ✅ `live_images`: 6개 저장 완료

---

## ⏰ 자동 수집 스케줄

### 수집 주기

**주기**: 매 시간 정각 (1시간 간격)

**실행 시간**: 
```
00:00, 01:00, 02:00, 03:00, 04:00, 05:00,
06:00, 07:00, 08:00, 09:00, 10:00, 11:00,
12:00, 13:00, 14:00, 15:00, 16:00, 17:00,
18:00, 19:00, 20:00, 21:00, 22:00, 23:00
```

**다음 수집 예정**: 09:00 (약 20분 후)

---

## 📦 수집 대상 플랫폼

### 활성 플랫폼 (10개)

1. ✅ **네이버** (NAVER) - https://shoppinglive.naver.com
2. ✅ **카카오** (KAKAO) - https://shoppinglive.kakao.com
3. ✅ **11번가** (11ST)
4. ✅ **G마켓** (GMARKET)
5. ✅ **올리브영** (OLIVEYOUNG)
6. ✅ **그립** (GRIP)
7. ✅ **무신사** (MUSINSA)
8. ✅ **롯데온** (LOTTEON)
9. ✅ **아모레몰** (AMOREMALL)
10. ✅ **이니스프리몰** (INNISFREE_MALL)

---

## 🔄 데이터 흐름

```
크롤러 스케줄러 (매 시간)
    ↓
네이버 쇼핑라이브 크롤링
    ↓
데이터 파싱 및 정제
    ↓
Supabase 저장
    ↓
백엔드 API (Redis 캐시 5분)
    ↓
프론트엔드 표시
```

---

## 📊 스케줄러 설정

### 실행 명령어

```bash
cd /Users/amore/ai_cs\ 시스템/crawler
nohup python3 dynamic_scheduler.py > logs/scheduler_$(date +%Y%m%d_%H%M%S).log 2>&1 &
```

### 프로세스 확인

```bash
# 스케줄러 프로세스 확인
ps aux | grep dynamic_scheduler.py

# 로그 실시간 확인
tail -f crawler/logs/scheduler_*.log
```

### 스케줄러 중지

```bash
# PID로 중지
kill 8848

# 또는 프로세스 이름으로 중지
pkill -f dynamic_scheduler.py
```

---

## 📝 로그 파일

### 로그 위치

**스케줄러 로그**: `crawler/logs/scheduler_YYYYMMDD_HHMMSS.log`

**크롤러 로그**: `crawler/logs/comprehensive_crawler_YYYYMMDD.log`

### 로그 확인

```bash
# 최신 로그 확인
tail -f crawler/logs/scheduler_*.log

# 특정 날짜 로그
cat crawler/logs/scheduler_20251208_083757.log

# 에러만 확인
grep ERROR crawler/logs/scheduler_*.log
```

---

## 🔧 수집 설정

### platforms.json

**파일**: `crawler/config/platforms.json`

**설정 예시**:
```json
[
  {
    "code": "NAVER",
    "name": "네이버",
    "url": "https://shoppinglive.naver.com",
    "isActive": true
  },
  {
    "code": "KAKAO",
    "name": "카카오",
    "url": "https://shoppinglive.kakao.com",
    "isActive": true
  }
]
```

### brands.json

**파일**: `crawler/config/brands.json`

**설정된 브랜드** (10개):
- 설화수, 라네즈, 아이오페, 헤라, 에스트라
- 이니스프리, 해피바스, 바이탈뷰티, 프리메라, 오설록

---

## 🎯 대시보드 데이터 표시

### 백엔드 API 캐싱

**Redis 캐시**: 5분

**데이터 갱신 주기**:
- 크롤러: 1시간마다 새 데이터 수집
- 백엔드: 5분마다 캐시 갱신
- 프론트엔드: 실시간 표시

### 대시보드 조회

**URL**: https://aics1.netlify.app

**표시 데이터**:
- ✅ 최신 라이브 방송 정보
- ✅ 상품 목록 (중복 제거)
- ✅ 혜택 정보
- ✅ CS 응대 정보
- ✅ 통계 데이터

---

## ⚠️ 알려진 이슈

### FAQ 저장 오류

**문제**: 
```
null value in column "faq_id" of relation "live_faqs" violates not-null constraint
```

**원인**: `faq_id` 컬럼이 NOT NULL이지만 값이 제공되지 않음

**영향**: FAQ 데이터가 저장되지 않음 (다른 데이터는 정상)

**해결 방법** (선택사항):

#### 옵션 1: 크롤러 수정
```python
# comprehensive_naver_crawler.py에서 faq_id 생성
import uuid

faq_data = {
    'faq_id': str(uuid.uuid4()),  # UUID 생성
    'live_id': live_id,
    'question': question,
    'answer': answer
}
```

#### 옵션 2: Supabase 스키마 수정
```sql
-- faq_id를 자동 생성되도록 변경
ALTER TABLE live_faqs 
ALTER COLUMN faq_id SET DEFAULT gen_random_uuid();
```

---

## 📈 수집 통계

### 현재 세션

**총 실행 횟수**: 1회

**성공**: 1회

**실패**: 0회

**수집된 데이터**:
- 라이브 방송: 1개
- 상품: 18개
- 쿠폰: 1개
- 댓글: 6개
- 이미지: 6개

### 다음 수집

**예정 시간**: 09:00 (매 시간 정각)

**수집 플랫폼**: 10개 플랫폼

---

## 🔄 스케줄러 관리

### 상태 확인

```bash
# 프로세스 확인
ps aux | grep dynamic_scheduler.py

# 로그 실시간 확인
tail -f /Users/amore/ai_cs\ 시스템/crawler/logs/scheduler_*.log
```

### 재시작

```bash
# 기존 프로세스 종료
pkill -f dynamic_scheduler.py

# 재시작
cd "/Users/amore/ai_cs 시스템/crawler"
nohup python3 dynamic_scheduler.py > logs/scheduler_$(date +%Y%m%d_%H%M%S).log 2>&1 &
```

### 수동 실행 (테스트용)

```bash
cd "/Users/amore/ai_cs 시스템/crawler"
python3 comprehensive_naver_crawler.py
```

---

## ✅ 완료 체크리스트

### 크롤러 설정

- [x] 크롤러 환경 변수 설정
- [x] Python 의존성 설치
- [x] Supabase 연결 확인
- [x] 크롤러 수동 실행 테스트
- [x] 데이터 수집 성공 확인

### 스케줄러 설정

- [x] 스케줄러 코드 확인
- [x] 1시간 주기 설정 확인
- [x] 백그라운드 실행
- [x] 로그 파일 생성 확인
- [x] 프로세스 실행 확인

### 데이터 흐름

- [x] Supabase 데이터 저장 확인
- [x] 백엔드 API 연동 (Render 배포 완료 후)
- [ ] 프론트엔드 데이터 표시 확인 (Render 재배포 후)

---

## 🎉 자동화 완료!

크롤러가 1시간 주기로 자동 실행되도록 설정되었습니다!

**스케줄러 상태**: ✅ 실행 중 (PID: 8848)

**다음 수집**: 09:00 (매 시간 정각)

**수집 플랫폼**: 10개

**데이터 저장**: Supabase 자동 저장

---

## 📞 관련 문서

- [크롤러_실행_가이드.md](./크롤러_실행_가이드.md)
- [네이버_쇼핑라이브_통합_가이드.md](./네이버_쇼핑라이브_통합_가이드.md)
- [데이터_수집_스케줄러_가이드.md](./데이터_수집_스케줄러_가이드.md)

